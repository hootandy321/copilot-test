#!/usr/bin/env python3
"""
QW (Qwen3) Small Matrix Test
åŸºäº InfiniCore-Infer-main/src/qw å†…çš„å‡½æ•°è¿›è¡Œå°çŸ©é˜µæµ‹è¯•

æµ‹è¯•ç›®æ ‡:
- ä½¿ç”¨å°ç»´åº¦çŸ©é˜µï¼Œèƒ½åœ¨ç»ˆç«¯é‡Œçœ‹å…¨
- å¯è‡ªå®šä¹‰æƒé‡ã€è¾“å…¥
- æ¯å±‚éƒ½æœ‰è¾“å‡º
- å’Œé¢„æµ‹ç»“æœå¯¹æ¯”ï¼Œæ‰¾å‡ºæœ‰é—®é¢˜çš„å±‚

åŸºäº Qwen3 æ¶æ„è¿›è¡Œç®€åŒ–çš„æ•°å­¦éªŒè¯æµ‹è¯•
"""

import numpy as np
import sys
import os
from typing import Dict, List, Tuple, Optional
import json
import math

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
np.random.seed(42)

class SmallQwen3Config:
    """å°çŸ©é˜µæµ‹è¯•é…ç½®"""
    def __init__(self):
        # æå°çš„æ¨¡å‹é…ç½®ï¼Œä¾¿äºåœ¨ç»ˆç«¯æ˜¾ç¤º
        self.nlayer = 2           # 2å±‚ transformer
        self.d = 8               # éšè—ç»´åº¦ 8 (å¯ä»¥æ˜¾ç¤º 8x8 çŸ©é˜µ)
        self.nh = 2              # 2ä¸ªæ³¨æ„åŠ›å¤´
        self.nkvh = 2            # K/Vå¤´æ•°é‡
        self.dh = self.d // self.nh  # æ¯ä¸ªå¤´çš„ç»´åº¦ = 4
        self.di = 16             # MLPä¸­é—´ç»´åº¦
        self.dctx = 4            # æœ€å¤§åºåˆ—é•¿åº¦
        self.dvoc = 10           # è¯æ±‡è¡¨å¤§å°
        self.epsilon = 1e-6      # RMSå½’ä¸€åŒ–å‚æ•°
        self.theta = 10000.0     # RoPEå‚æ•°
        
        print(f"Small Qwen3 Config:")
        print(f"  - Layers: {self.nlayer}")
        print(f"  - Hidden dim: {self.d}")  
        print(f"  - Attention heads: {self.nh}")
        print(f"  - Head dim: {self.dh}")
        print(f"  - MLP intermediate: {self.di}")
        print(f"  - Max sequence: {self.dctx}")
        print(f"  - Vocabulary: {self.dvoc}")

class QwenLayer:
    """å•ä¸ª Qwen3 å±‚çš„å®ç°ï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, config: SmallQwen3Config, layer_id: int):
        self.config = config
        self.layer_id = layer_id
        self.d = config.d
        self.nh = config.nh
        self.nkvh = config.nkvh  
        self.dh = config.dh
        self.di = config.di
        
        # åˆå§‹åŒ–æƒé‡ - ä½¿ç”¨å°çš„éšæœºå€¼ä¾¿äºè§‚å¯Ÿ
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–å±‚æƒé‡"""
        print(f"\n=== Layer {self.layer_id} Weight Initialization ===")
        
        # Attentionæƒé‡
        self.w_attn_norm = self._init_norm_weight("attn_norm", self.d)
        self.w_q_proj = self._init_linear_weight("q_proj", self.d, self.nh * self.dh) 
        self.w_k_proj = self._init_linear_weight("k_proj", self.d, self.nkvh * self.dh)
        self.w_v_proj = self._init_linear_weight("v_proj", self.d, self.nkvh * self.dh)
        self.w_o_proj = self._init_linear_weight("o_proj", self.nh * self.dh, self.d)
        
        # Qwen3ç‰¹æœ‰çš„Q/K normalization  
        self.w_q_norm = self._init_norm_weight("q_norm", self.dh)
        self.w_k_norm = self._init_norm_weight("k_norm", self.dh)
        
        # MLPæƒé‡
        self.w_mlp_norm = self._init_norm_weight("mlp_norm", self.d)
        self.w_gate_proj = self._init_linear_weight("gate_proj", self.d, self.di)
        self.w_up_proj = self._init_linear_weight("up_proj", self.d, self.di)  
        self.w_down_proj = self._init_linear_weight("down_proj", self.di, self.d)
        
    def _init_norm_weight(self, name: str, dim: int) -> np.ndarray:
        """åˆå§‹åŒ–å½’ä¸€åŒ–å±‚æƒé‡"""
        # å½’ä¸€åŒ–å±‚æƒé‡åˆå§‹åŒ–ä¸ºæ¥è¿‘1çš„å€¼
        weight = np.ones(dim) + np.random.randn(dim) * 0.01
        print(f"  {name}: shape={weight.shape}, mean={weight.mean():.4f}, std={weight.std():.4f}")
        return weight
        
    def _init_linear_weight(self, name: str, in_dim: int, out_dim: int) -> np.ndarray:
        """åˆå§‹åŒ–çº¿æ€§å±‚æƒé‡"""
        # ä½¿ç”¨Xavieråˆå§‹åŒ–ä½†ç¼©å°æ–¹å·®ä¾¿äºè§‚å¯Ÿ
        weight = np.random.randn(out_dim, in_dim) * (0.5 / np.sqrt(in_dim))
        print(f"  {name}: shape={weight.shape}, mean={weight.mean():.4f}, std={weight.std():.4f}")
        return weight

    def rms_norm(self, x: np.ndarray, weight: np.ndarray, eps: float = 1e-6) -> np.ndarray:
        """RMSå½’ä¸€åŒ–"""
        norm = np.linalg.norm(x, axis=-1, keepdims=True)
        rms = norm * (x.shape[-1] ** -0.5)
        return (x / (rms + eps)) * weight

    def rope_embeddings(self, q: np.ndarray, k: np.ndarray, seq_len: int) -> Tuple[np.ndarray, np.ndarray]:
        """æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)"""
        # ç®€åŒ–ç‰ˆæœ¬çš„RoPEï¼Œä»…ç”¨äºæµ‹è¯•
        dim = q.shape[-1]
        # åˆ›å»ºä½ç½®ç¼–ç 
        positions = np.arange(seq_len, dtype=np.float32)
        freqs = 1.0 / (self.config.theta ** (np.arange(0, dim, 2, dtype=np.float32) / dim))
        
        # ç”Ÿæˆsin/cosè¡¨
        outer = np.outer(positions, freqs)
        cos_vals = np.cos(outer)  
        sin_vals = np.sin(outer)
        
        # åº”ç”¨æ—‹è½¬ (ç®€åŒ–ç‰ˆæœ¬)
        def apply_rope(tensor):
            # è¿™é‡Œå®ç°ä¸€ä¸ªç®€åŒ–çš„æ—‹è½¬æ“ä½œ
            # å®é™…çš„RoPEæ›´å¤æ‚ï¼Œä½†è¿™è¶³ä»¥æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            return tensor  # æš‚æ—¶è¿”å›åŸå€¼ï¼Œä¸»è¦æµ‹è¯•å…¶ä»–å±‚
            
        return apply_rope(q), apply_rope(k)

    def multi_head_attention(self, hidden_states: torch.Tensor, layer_output: Dict) -> torch.Tensor:
        """å¤šå¤´æ³¨æ„åŠ›è®¡ç®—"""
        seq_len, hidden_dim = hidden_states.shape
        
        print(f"\n  --- Multi-Head Attention (Layer {self.layer_id}) ---")
        print(f"  Input shape: {hidden_states.shape}")
        
        # è¾“å…¥å½’ä¸€åŒ–
        normed_input = self.rms_norm(hidden_states, self.w_attn_norm, self.config.epsilon)
        layer_output[f"attn_norm"] = normed_input
        print(f"  After attention norm: shape={normed_input.shape}, mean={normed_input.mean():.4f}")
        self._print_small_matrix("Attention norm output", normed_input)
        
        # Q, K, VæŠ•å½±
        q = torch.mm(normed_input, self.w_q_proj.t())  # [seq_len, nh * dh]
        k = torch.mm(normed_input, self.w_k_proj.t())  # [seq_len, nkvh * dh] 
        v = torch.mm(normed_input, self.w_v_proj.t())  # [seq_len, nkvh * dh]
        
        layer_output[f"q_proj"] = q
        layer_output[f"k_proj"] = k
        layer_output[f"v_proj"] = v
        
        print(f"  Q projection: shape={q.shape}, mean={q.mean():.4f}")
        print(f"  K projection: shape={k.shape}, mean={k.mean():.4f}") 
        print(f"  V projection: shape={v.shape}, mean={v.mean():.4f}")
        
        self._print_small_matrix("Q projection", q)
        self._print_small_matrix("K projection", k)
        self._print_small_matrix("V projection", v)
        
        # é‡æ–°æ•´å½¢ä¸ºå¤šå¤´æ ¼å¼
        q = q.view(seq_len, self.nh, self.dh)  
        k = k.view(seq_len, self.nkvh, self.dh)
        v = v.view(seq_len, self.nkvh, self.dh)
        
        # Qwen3ç‰¹æœ‰çš„Q/Kå½’ä¸€åŒ–
        q_norm_reshaped = self.rms_norm(q.view(-1, self.dh), self.w_q_norm).view(seq_len, self.nh, self.dh)
        k_norm_reshaped = self.rms_norm(k.view(-1, self.dh), self.w_k_norm).view(seq_len, self.nkvh, self.dh)
        
        layer_output[f"q_norm"] = q_norm_reshaped
        layer_output[f"k_norm"] = k_norm_reshaped
        
        print(f"  Q after norm: shape={q_norm_reshaped.shape}, mean={q_norm_reshaped.mean():.4f}")
        print(f"  K after norm: shape={k_norm_reshaped.shape}, mean={k_norm_reshaped.mean():.4f}")
        
        # RoPEä½ç½®ç¼–ç 
        q_rope, k_rope = self.rope_embeddings(q_norm_reshaped, k_norm_reshaped, seq_len)
        layer_output[f"q_rope"] = q_rope
        layer_output[f"k_rope"] = k_rope
        
        # æ³¨æ„åŠ›è®¡ç®—
        # è½¬ç½®ä»¥ä¾¿è®¡ç®—: [seq_len, nh, dh] -> [nh, seq_len, dh]
        q_t = q_rope.transpose(0, 1)  # [nh, seq_len, dh]
        k_t = k_rope.transpose(0, 1)  # [nkvh, seq_len, dh] 
        v_t = v.transpose(0, 1)       # [nkvh, seq_len, dh]
        
        # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®— (ç®€åŒ–ç‰ˆæœ¬ï¼Œå‡è®¾nh == nkvh)
        attn_scores = torch.matmul(q_t, k_t.transpose(-2, -1)) / np.sqrt(self.dh)
        attn_weights = F.softmax(attn_scores, dim=-1)
        layer_output[f"attn_weights"] = attn_weights
        
        print(f"  Attention scores: shape={attn_scores.shape}, mean={attn_scores.mean():.4f}")
        print(f"  Attention weights: shape={attn_weights.shape}, sum={attn_weights.sum(-1).mean():.4f}")
        
        # æ³¨æ„åŠ›åº”ç”¨
        attn_output = torch.matmul(attn_weights, v_t)  # [nh, seq_len, dh]
        attn_output = attn_output.transpose(0, 1)  # [seq_len, nh, dh] 
        attn_output = attn_output.contiguous().view(seq_len, self.nh * self.dh)  # [seq_len, nh * dh]
        layer_output[f"attn_output"] = attn_output
        
        print(f"  Attention output: shape={attn_output.shape}, mean={attn_output.mean():.4f}")
        self._print_small_matrix("Attention output", attn_output)
        
        # è¾“å‡ºæŠ•å½±
        output = torch.mm(attn_output, self.w_o_proj.t())  # [seq_len, d]
        layer_output[f"attn_final"] = output
        
        print(f"  Output projection: shape={output.shape}, mean={output.mean():.4f}")
        self._print_small_matrix("Attention final output", output)
        
        # æ®‹å·®è¿æ¥
        output = output + hidden_states
        layer_output[f"attn_residual"] = output
        print(f"  After residual: shape={output.shape}, mean={output.mean():.4f}")
        
        return output

    def mlp_layer(self, hidden_states: torch.Tensor, layer_output: Dict) -> torch.Tensor:
        """MLPå±‚è®¡ç®—"""
        print(f"\n  --- MLP Layer (Layer {self.layer_id}) ---")
        print(f"  Input shape: {hidden_states.shape}")
        
        # MLPå½’ä¸€åŒ–
        normed_input = self.rms_norm(hidden_states, self.w_mlp_norm, self.config.epsilon)
        layer_output[f"mlp_norm"] = normed_input
        print(f"  After MLP norm: shape={normed_input.shape}, mean={normed_input.mean():.4f}")
        self._print_small_matrix("MLP norm output", normed_input)
        
        # Gateå’ŒUpæŠ•å½±
        gate_output = torch.mm(normed_input, self.w_gate_proj.t())  # [seq_len, di]
        up_output = torch.mm(normed_input, self.w_up_proj.t())      # [seq_len, di]
        
        layer_output[f"gate_proj"] = gate_output
        layer_output[f"up_proj"] = up_output
        
        print(f"  Gate projection: shape={gate_output.shape}, mean={gate_output.mean():.4f}")
        print(f"  Up projection: shape={up_output.shape}, mean={up_output.mean():.4f}")
        
        # SwiGLUæ¿€æ´»å‡½æ•°
        gate_activated = F.silu(gate_output)  # SiLUæ¿€æ´»
        mlp_output = gate_activated * up_output
        layer_output[f"mlp_intermediate"] = mlp_output
        
        print(f"  After SwiGLU: shape={mlp_output.shape}, mean={mlp_output.mean():.4f}")
        self._print_small_matrix("MLP intermediate", mlp_output)
        
        # DownæŠ•å½±
        output = torch.mm(mlp_output, self.w_down_proj.t())  # [seq_len, d]
        layer_output[f"mlp_output"] = output
        
        print(f"  Down projection: shape={output.shape}, mean={output.mean():.4f}")
        self._print_small_matrix("MLP output", output)
        
        # æ®‹å·®è¿æ¥
        output = output + hidden_states  
        layer_output[f"mlp_residual"] = output
        print(f"  After residual: shape={output.shape}, mean={output.mean():.4f}")
        
        return output

    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """å‰å‘ä¼ æ’­"""
        layer_output = {}
        
        print(f"\n{'='*60}")
        print(f"Layer {self.layer_id} Forward Pass")
        print(f"{'='*60}")
        
        # å¤šå¤´æ³¨æ„åŠ›
        attn_output = self.multi_head_attention(hidden_states, layer_output)
        
        # MLPå±‚
        mlp_output = self.mlp_layer(attn_output, layer_output)
        
        return mlp_output, layer_output
    
    def _print_small_matrix(self, name: str, tensor: torch.Tensor, max_elements: int = 64):
        """æ‰“å°å°çŸ©é˜µï¼Œä¾¿äºåœ¨ç»ˆç«¯æŸ¥çœ‹"""
        if tensor.numel() <= max_elements:
            print(f"  {name}:")
            if tensor.dim() == 1:
                print(f"    {tensor.detach().numpy()}")
            elif tensor.dim() == 2:
                for i, row in enumerate(tensor.detach().numpy()):
                    print(f"    [{i}] {row}")
            else:
                print(f"    Shape: {tensor.shape}, too many dimensions to display")
        else:
            print(f"  {name}: shape={tensor.shape} (too large to display)")


class SmallQwen3Model:
    """å°çŸ©é˜µ Qwen3 æ¨¡å‹"""
    
    def __init__(self, config: SmallQwen3Config):
        self.config = config
        self.layers = []
        
        # åˆå§‹åŒ–å„å±‚
        for i in range(config.nlayer):
            layer = QwenLayer(config, i)
            self.layers.append(layer)
        
        # è¾“å…¥åµŒå…¥å’Œè¾“å‡ºå±‚
        self.input_embedding = torch.randn(config.dvoc, config.d) * 0.1
        self.output_norm = torch.ones(config.d) + torch.randn(config.d) * 0.01
        self.output_projection = torch.randn(config.d, config.dvoc) * 0.1
        
        print(f"\nModel initialized with {len(self.layers)} layers")

    def embed_tokens(self, token_ids: List[int]) -> torch.Tensor:
        """TokenåµŒå…¥"""
        embeddings = []
        for token_id in token_ids:
            if token_id >= self.config.dvoc:
                token_id = token_id % self.config.dvoc  # ç®€å•æˆªæ–­
            embeddings.append(self.input_embedding[token_id])
        return torch.stack(embeddings)

    def forward(self, token_ids: List[int]) -> Tuple[torch.Tensor, Dict]:
        """å®Œæ•´çš„å‰å‘ä¼ æ’­"""
        print(f"\n{'#'*80}")
        print(f"Qwen3 Model Forward Pass")
        print(f"Input tokens: {token_ids}")
        print(f"{'#'*80}")
        
        # TokenåµŒå…¥
        hidden_states = self.embed_tokens(token_ids)
        print(f"\nInput embeddings: shape={hidden_states.shape}, mean={hidden_states.mean():.4f}")
        
        # å­˜å‚¨æ¯å±‚è¾“å‡ºç”¨äºå¯¹æ¯”
        all_layer_outputs = {}
        
        # é€å±‚è®¡ç®—
        for i, layer in enumerate(self.layers):
            hidden_states, layer_output = layer.forward(hidden_states)
            all_layer_outputs[f"layer_{i}"] = layer_output
            
            print(f"\nLayer {i} final output: shape={hidden_states.shape}, mean={hidden_states.mean():.4f}")
            layer._print_small_matrix(f"Layer {i} output", hidden_states)
        
        # æœ€ç»ˆè¾“å‡ºå½’ä¸€åŒ–
        final_norm = layer.rms_norm(hidden_states, self.output_norm, self.config.epsilon)
        print(f"\nFinal norm: shape={final_norm.shape}, mean={final_norm.mean():.4f}")
        layer._print_small_matrix("Final norm", final_norm)
        
        # è¾“å‡ºæŠ•å½±åˆ°è¯æ±‡è¡¨
        logits = torch.mm(final_norm, self.output_projection.t())
        print(f"Final logits: shape={logits.shape}, mean={logits.mean():.4f}")
        layer._print_small_matrix("Final logits", logits)
        
        return logits, all_layer_outputs

class QwenTester:
    """Qwenæ¨¡å‹æµ‹è¯•å™¨ï¼Œç”¨äºéªŒè¯å±‚çº§è®¡ç®—"""
    
    def __init__(self):
        self.config = SmallQwen3Config()
        self.model = SmallQwen3Model(self.config)
    
    def run_test(self, test_tokens: Optional[List[int]] = None):
        """è¿è¡Œæµ‹è¯•"""
        if test_tokens is None:
            # é»˜è®¤æµ‹è¯•åºåˆ—
            test_tokens = [1, 3, 7, 2]  # ç¡®ä¿åœ¨è¯æ±‡è¡¨èŒƒå›´å†…
        
        print(f"\n{'='*80}")
        print(f"QW Small Matrix Test Started")
        print(f"Test Configuration:")
        print(f"  - Test tokens: {test_tokens}")
        print(f"  - Sequence length: {len(test_tokens)}")
        print(f"{'='*80}")
        
        # è¿è¡Œå‰å‘ä¼ æ’­
        try:
            logits, layer_outputs = self.model.forward(test_tokens)
            
            # åˆ†æç»“æœ
            self.analyze_results(logits, layer_outputs, test_tokens)
            
            return True
            
        except Exception as e:
            print(f"âŒ Test failed with error: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def analyze_results(self, logits: torch.Tensor, layer_outputs: Dict, test_tokens: List[int]):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"Results Analysis")
        print(f"{'='*80}")
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
        final_probs = F.softmax(logits[-1], dim=-1)  # æœ€åä¸€ä¸ªä½ç½®çš„æ¦‚ç‡
        predicted_token = torch.argmax(final_probs).item()
        
        print(f"\nFinal predictions:")
        print(f"  Input tokens: {test_tokens}")
        print(f"  Predicted next token: {predicted_token}")
        print(f"  Prediction confidence: {final_probs[predicted_token]:.4f}")
        print(f"  Top 3 predictions:")
        
        top_k_probs, top_k_indices = torch.topk(final_probs, 3)
        for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):
            print(f"    {i+1}. Token {idx.item()}: {prob:.4f}")
        
        # æ£€æŸ¥æ¯å±‚çš„æ•°å€¼ç¨³å®šæ€§
        print(f"\nNumerical Stability Check:")
        for layer_name, layer_data in layer_outputs.items():
            print(f"\n  {layer_name}:")
            for key, tensor in layer_data.items():
                if isinstance(tensor, torch.Tensor):
                    mean_val = tensor.mean().item()
                    std_val = tensor.std().item()
                    max_val = tensor.max().item()
                    min_val = tensor.min().item()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
                    issues = []
                    if abs(mean_val) > 10:
                        issues.append("large_mean")
                    if std_val > 10:
                        issues.append("large_variance")
                    if torch.isnan(tensor).any():
                        issues.append("NaN_values")
                    if torch.isinf(tensor).any():
                        issues.append("Inf_values")
                    
                    status = "âŒ " + ",".join(issues) if issues else "âœ… OK"
                    print(f"    {key}: mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f} {status}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
        self.save_results_to_json(logits, layer_outputs, test_tokens)
    
    def save_results_to_json(self, logits: torch.Tensor, layer_outputs: Dict, test_tokens: List[int]):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æ"""
        results = {
            "config": {
                "nlayer": self.config.nlayer,
                "d": self.config.d,
                "nh": self.config.nh,
                "dh": self.config.dh,
                "di": self.config.di,
                "dctx": self.config.dctx,
                "dvoc": self.config.dvoc
            },
            "test_input": {
                "tokens": test_tokens,
                "sequence_length": len(test_tokens)
            },
            "final_logits": logits.detach().numpy().tolist(),
            "layer_statistics": {}
        }
        
        # æ”¶é›†æ¯å±‚ç»Ÿè®¡ä¿¡æ¯
        for layer_name, layer_data in layer_outputs.items():
            results["layer_statistics"][layer_name] = {}
            for key, tensor in layer_data.items():
                if isinstance(tensor, torch.Tensor):
                    results["layer_statistics"][layer_name][key] = {
                        "shape": list(tensor.shape),
                        "mean": tensor.mean().item(),
                        "std": tensor.std().item(),
                        "min": tensor.min().item(),
                        "max": tensor.max().item()
                    }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = "/tmp/qw_test_results.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ“ Detailed results saved to: {output_file}")

    def compare_with_expected(self, expected_file: Optional[str] = None):
        """ä¸é¢„æœŸç»“æœå¯¹æ¯” (å¦‚æœæœ‰çš„è¯)"""
        if expected_file and os.path.exists(expected_file):
            print(f"\nComparing with expected results from {expected_file}")
            # è¿™é‡Œå¯ä»¥å®ç°ä¸é¢„æœŸç»“æœçš„å¯¹æ¯”é€»è¾‘
        else:
            print(f"\nNo expected results file provided for comparison")

def main():
    """ä¸»å‡½æ•°"""
    print("QW (Qwen3) Small Matrix Test")
    print("Based on InfiniCore-Infer-main/src/qw functions\n")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = QwenTester()
    
    # è¿è¡ŒåŸºæœ¬æµ‹è¯•
    success = tester.run_test()
    
    # å¯ä»¥è¿è¡Œå¤šä¸ªä¸åŒçš„æµ‹è¯•ç”¨ä¾‹
    if success:
        print(f"\n{'='*60}")
        print("Running additional test cases...")
        print(f"{'='*60}")
        
        # æµ‹è¯•æ›´é•¿åºåˆ—
        tester.run_test([0, 1, 2, 3])
        
        # æµ‹è¯•å•ä¸ªtoken
        tester.run_test([5])
        
        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        tester.run_test([9, 0, 1])  # åŒ…å«æœ€å¤§è¯æ±‡ID
    
    print(f"\nQW Matrix Test Completed!")
    print(f"Check /tmp/qw_test_results.json for detailed analysis")

if __name__ == "__main__":
    main()